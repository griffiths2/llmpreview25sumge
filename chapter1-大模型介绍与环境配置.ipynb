{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c7fc38",
   "metadata": {},
   "source": [
    "# Chapter-1 大模型介绍与环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b1583",
   "metadata": {},
   "source": [
    "本节学习内容大纲\n",
    "\n",
    "- 大模型基本概念\n",
    "  - NLP的传统任务有哪些？\n",
    "  - 什么是大模型？\n",
    "- 大模型与传统NLP模型（Bert）有什么不同？\n",
    "- 动手实践\n",
    "  - 服务器环境配置（transformers, modelscope）\n",
    "  - 模型下载（Hugging Face, ModelScope）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44441c",
   "metadata": {},
   "source": [
    "大家好，\n",
    "\n",
    "欢迎来到大模型使用和学习的入门课程。这门课是为像你们一样，刚刚踏入计算机科学领域的同学设计的。我们不会讲非常复杂的理论，而是从最实际的角度出发，带大家了解什么是大模型，以及如何动手使用它们。\n",
    "\n",
    "今天，我们的主要任务有两个：***第一，建立对大模型的基本认知；第二，把我们将来做实验要用的计算机环境配置好。***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854f36f",
   "metadata": {},
   "source": [
    "## 1.1 大模型基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd3e3c",
   "metadata": {},
   "source": [
    "在我们直接进入“大模型”这个话题之前，我们先花几分钟了解一下它所属的领域：自然语言处理（NLP）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4086f7",
   "metadata": {},
   "source": [
    "### 1.1.1 NLP的传统任务有哪些？\n",
    "\n",
    "NLP，Natural Language Processing，自然语言处理，是人工智能的一个分支，它的目标是让计算机能够理解、解释和生成人类语言。NLP 结合了计算机科学、人工智能、语言学和心理学等多个学科的知识和技术，旨在打破人类语言和计算机语言之间的障碍，实现无缝的交流与互动。\n",
    "\n",
    "在最近几年大模型火起来之前，NLP领域是由很多个“专项技能”组成的。就像一个公司里有管财务的，有管销售的，有管人事的，各司其职。\n",
    "\n",
    "传统的NLP任务主要有下面这些，我举几个大家可能接触过的例子：\n",
    "\n",
    "- 文本分类 (Text Classification): 给一段文本打上预设的标签。\n",
    "```\n",
    "例子1：垃圾邮件识别。 你的邮箱自动把一封邮件识别为“垃圾邮件”或“正常邮件”。\n",
    "例子2：情感分析。 分析一条电影评论“这部电影太棒了，演员演技都在线！”的情感是“正面”的，还是“负面”的。\n",
    "```\n",
    "\n",
    "- 命名实体识别 (Named Entity Recognition, NER): 从文本中找出特定信息，比如人名、地名、组织机构名等。\n",
    "```\n",
    "例子： “史蒂夫·乔布斯在加利福尼亚州的库比蒂诺创立了苹果公司。”\n",
    "这个任务会识别出：史蒂夫·乔布斯 (人名)，加利福尼亚州 (地名)，库比蒂诺 (地名)，苹果公司 (组织机构名)。这在信息提取、构建知识图谱时非常有用。\n",
    "```\n",
    "\n",
    "- 机器翻译 (Machine Translation): 把一种语言自动翻译成另一种语言。\n",
    "```\n",
    "例子： 这个大家最熟悉，比如把 \"Hello, world!\" 翻译成 “你好，世界！”。\n",
    "```\n",
    "\n",
    "- 问答系统 (Question Answering): 给定一个问题和一段背景材料，让机器从中找出答案。\n",
    "```\n",
    "例子： 背景材料是某产品的说明书，你提问：“这款手机的电池容量是多少？”，系统能准确回答：“5000mAh”。\n",
    "```\n",
    "\n",
    "> 注：如果想要更详细的了解NLP的任务，可参考 [Happy-LLM Chapter-1 NLP 基础概念](https://github.com/datawhalechina/happy-llm/blob/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20NLP%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5.md)\n",
    "\n",
    "在过去，要完成上面任何一个任务，我们通常都需要一个专门为此任务训练的模型。一个做翻译的模型，基本不会做情感分析。这就是传统NLP模型的工作模式：**专业，一模一用**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d297f",
   "metadata": {},
   "source": [
    "### 1.1.2 什么是大模型\n",
    "\n",
    "那么，什么是大模型（Large Language Model, LLM）呢？从字面上理解，“大”是它最直观的特点。这个“大”体现在两个方面：\n",
    "\n",
    "- 参数规模大： 模型的参数量巨大，通常从 70亿（7B） 级别起步，到现在已经有千亿、万亿 （DeepSeek-R1：671B，GPT-4：约1.8T） 级别的模型。你可以把参数想象成模型大脑中的神经元连接点，连接点越多，模型就越“聪明”，能记住和学习的知识就越多。\n",
    "- 训练数据量大： 大模型通常使用了海量的文本数据进行训练，这些数据可能包含了互联网上大部分高质量的公开文本。\n",
    "\n",
    "但大模型不仅仅是“大”而已。它是一种通用的、预训练过的语言模型。它在训练时，目标非常单纯：根据前面的内容，预测下一个最可能出现的词（token）是什么。\n",
    "\n",
    "> 注：token 是指模型能够识别的最小单位，比如一个单词、一个标点符号等。在大模型的训练中，模型会将文本中的每个 token 都映射成一个数字，然后模型会根据这些数字来预测下一个最可能出现的 token。\n",
    "\n",
    "通过在海量数据上日复一日地做这个“完形填空”或“接龙”游戏，模型慢慢学会了语法、事实知识、逻辑推理，甚至某种程度上的“世界观”。因为它学的知识是通用的，所以它不再是上面提到的“专才”，而更像一个**“通才”**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fab20e",
   "metadata": {},
   "source": [
    "### 1.1.3 大模型与传统NLP模型（如BERT）有什么不同？\n",
    "\n",
    "为了让大家更清楚地理解这种变化，我们拿一个非常有代表性的传统NLP模型BERT，来和现在的大模型（比如 Qwen3 系列）做个对比。\n",
    "\n",
    "| 特性        | 传统 NLP 模型 (以 BERT 为例)         | 大语言模型 (以 Qwen3 系列为例)               |\n",
    "| --------- | ----------------------------- | -------------------------------- |\n",
    "| **模型规模**  | 亿级参数 (e.g., BERT-base: 1.1 亿) | 十亿到万亿级参数 (e.g., Qwen3-235B-A22B: 2350 亿)   |\n",
    "| **核心思想**  | 专业模式：针对特定任务进行微调 (Fine-tuning) | 通用模式：通过提示 (Prompting) 来解决各种任务    |\n",
    "| **使用方式**  | 需要在特定任务的数据集上进一步训练模型           | 基本不需要额外训练，直接通过给指令的方式使用           |\n",
    "| **任务灵活性** | 一个模型主要服务一个或少数几个任务             | 一个模型可以完成开放、多样的任务 (对话、写作、翻译、写代码等) |\n",
    "\n",
    "简单来说，最大的区别在于使用范式的改变：\n",
    "\n",
    "- BERT：像一个大学毕业生，基础不错（经过了预训练），但要胜任某个具体工作（比如情感分析），还需要岗前培训（用情感分析的数据集去微调）。\n",
    "- 大模型：像一个经验丰富的行业专家，你不需要再培训他。你只需要用自然语言给他下达指令（我们称之为“写提示词”），他就能直接开始工作。你想让他做情感分析，就问他“这段话是积极的还是消极的？”；你想让他翻译，就说“把这句话翻译成英文”。\n",
    "\n",
    "这种从“微调”到“提示”的转变，极大地降低了NLP技术的使用门槛，也是大模型如此强大的核心原因之一。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8dc3b8",
   "metadata": {},
   "source": [
    "## 1.2 环境配置\n",
    "\n",
    "概念部分我们先了解到这里。接下来，我们来做点实际操作。作为理工科专业的学生，动手能力是第一位的。我们将一步步配置好一个可以在服务器上运行大模型的基础环境。\n",
    "\n",
    "我们假设你已经有了一台 Linux 服务器的访问权限，并且这台服务器配备了 NVIDIA 的 GPU，并且已经安装好了可使用 CUDA 加速的 Pytorch。\n",
    "\n",
    "可以运行以下命令查看当前的 CUDA 版本：\n",
    "\n",
    "```bash\n",
    "!nvcc --version\n",
    "```\n",
    "\n",
    "可以运行以下命令查看当前的 Pytorch 版本：\n",
    "\n",
    "```bash\n",
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "测试一下 CUDA 是否可用：\n",
    "\n",
    "```bash\n",
    "!python -c \"import torch; print(torch.cuda.is_available())\"\n",
    "```\n",
    "\n",
    "如果输出 `True`，则说明 CUDA 已经成功配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b26132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Mar_28_02:18:24_PDT_2024\n",
      "Cuda compilation tools, release 12.4, V12.4.131\n",
      "Build cuda_12.4.r12.4/compiler.34097967_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9757fe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu126\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc23a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.cuda.is_available())\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c6267",
   "metadata": {},
   "source": [
    "### 1.2.1 配置模型下载和运行环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5b67e",
   "metadata": {},
   "source": [
    "Hugging Face 是全球最大的机器学习模型开源社区，提供了大量预训练模型、数据集和工具：\n",
    "\n",
    "- 拥有超过50万个开源模型\n",
    "- 提供完整的模型训练、部署和应用解决方案\n",
    "- 支持 Transformers、Diffusers 等主流框架\n",
    "- 有完善的文档和社区支持\n",
    "\n",
    "ModelScope 是阿里云推出的模型开源社区平台：\n",
    "\n",
    "- 提供中文生态下的各类AI模型\n",
    "- 包含NLP、CV、语音等多个领域的预训练模型\n",
    "- 支持一键部署和在线体验\n",
    "- 提供模型训练和推理的完整工具链\n",
    "\n",
    "接下来我们将介绍如何在这两个平台下载和使用预训练模型，大家可以任选自己喜欢的平台或方式下载模型～\n",
    "\n",
    "> Hugging Face : https://huggingface.co/  \n",
    "> ModelScope : https://www.modelscope.cn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91919932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置 pip 镜像源\n",
    "!pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce64709",
   "metadata": {},
   "source": [
    "#### Hugging Face 下载模型\n",
    "\n",
    "使用`huggingface`官方提供的`huggingface-cli`命令行工具。安装依赖:\n",
    "\n",
    "```shell\n",
    "pip install -U huggingface_hub\n",
    "```\n",
    "\n",
    "然后新建python文件，填入以下代码，运行即可。\n",
    "\n",
    "- resume-download：断点续下\n",
    "- local-dir：本地存储路径。（linux环境下需要填写绝对路径）\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# 设置国内下载镜像地址\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "# 下载模型\n",
    "os.system('huggingface-cli download --resume-download Qwen/Qwen3-0.6B --local-dir your_path')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8daf088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置国内下载镜像地址\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "# 下载模型\n",
    "os.system('huggingface-cli download --resume-download Qwen/Qwen3-0.6B --local-dir /root/autodl-tmp/model/Qwen/Qwen3-0.6B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20523297",
   "metadata": {},
   "source": [
    "#### Modelscope\n",
    "\n",
    "使用`modelscope`中的`snapshot_download`函数下载模型，第一个参数为模型名称，参数`cache_dir`为模型的下载路径。\n",
    "\n",
    "注意：`cache_dir`最好为绝对路径。\n",
    "\n",
    "安装依赖：\n",
    "  \n",
    "```shell\n",
    "pip install modelscope\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "在当前目录下新建python文件，填入以下代码，运行即可。\n",
    "\n",
    "```python\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "\n",
    "model_dir = snapshot_download('Qwen/Qwen3-0.6B', cache_dir='your path', revision='master')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4192dd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q modelscope transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6f3de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/autodl-tmp/model/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 15:06:10,411 - modelscope - INFO - Got 10 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82345cc97c35405f823eb1e5176462af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 10 items:   0%|          | 0.00/10.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548d00fa10b54a0aba160e69969fe43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/13.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0732de4e896c4b9c8a4d8294552a8c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/1.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fae691dc614b7a889cd582506112ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [merges.txt]:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d595ad28024ee3842a027a6d5f6a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [LICENSE]:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80067628bad340c3921ecce921e69d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556d8533749144e4a30af33c5e88b579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de28ede5f554dbeaf1eeea5e79133bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9f23dfe2f94d388dccc4ed12a4885a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/10.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13be4ea8d274a2d9c398466f17af437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/9.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac26992dddd74933ba3e196f929d68ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.json]:   0%|          | 0.00/2.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 15:07:18,863 - modelscope - INFO - Download model 'Qwen/Qwen3-0.6B' successfully.\n",
      "2025-09-17 15:07:18,866 - modelscope - INFO - Creating symbolic link [/root/autodl-tmp/model/Qwen/Qwen3-0.6B].\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "\n",
    "model_dir = snapshot_download('Qwen/Qwen3-0.6B', cache_dir='/root/autodl-tmp/model', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69281a07",
   "metadata": {},
   "source": [
    "接下来，我们使用 transformers 库来加载模型，并进行推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60808da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "好的，用户让我介绍中国台湾歌手张韶涵2025觅光演唱会。首先，我需要确认演唱会的具体时间，用户提到的是2025年，但可能需要确认是否准确。不过，通常张韶涵的演唱会时间可能在2025年，但需要核实。接下来，我需要准备介绍的内容，包括演唱会的基本信息、张韶涵的背景、演唱会的亮点、活动安排以及可能的观众期待等。同时，要确保信息准确，如果有不确定的地方，可能需要提醒用户确认时间或提供更多细节。另外，用户可能希望了解演唱会的意义或观众的反应，所以可以适当加入相关元素。最后，保持回答的简洁和专业，结构清晰，方便用户快速获取所需信息。\n",
      "</think>\n",
      "content: 张韶涵2025觅光演唱会将于**11月16日**在**台北市立美术馆**举行。作为台湾知名歌手，张韶涵以其深情的音乐与独特的艺术风格深受粉丝喜爱，此次演唱会将延续其音乐与艺术的结合，展现更多作品与舞台魅力。\n",
      "\n",
      "**演唱会亮点：**\n",
      "1. **音乐阵容**：特邀台湾知名音乐人、台湾流行音乐人及台湾本土艺术家参与，为观众带来多元化的音乐体验。\n",
      "2. **舞台表现**：张韶涵将亲自登台，结合舞台设计与灯光效果，展现个人风格与艺术表达。\n",
      "3. **互动环节**：现场可能设有音乐互动、观众分享环节，增强参与感。\n",
      "\n",
      "**观众期待：**\n",
      "- 深情的音乐与舞台表演，期待张韶涵在本次演出中展现更多个人魅力。\n",
      "- 台湾本土音乐与艺术的融合，为粉丝带来更丰富的文化体验。\n",
      "\n",
      "如需进一步了解演唱会详情，可联系官方活动信息或关注社交媒体动态。\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的transformers库组件\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 设置模型本地路径\n",
    "model_name = \"/root/autodl-tmp/model/Qwen/Qwen3-0___6B\"\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",  # 自动选择合适的数据类型\n",
    "    device_map=\"cuda:0\",    # 自动选择可用设备(CPU/GPU)\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 准备模型输入\n",
    "prompt = \"你好，请介绍中国台湾歌手张韶涵2025觅光演唱会\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # 选择是否打开深度推理模式\n",
    ")\n",
    "# 将输入文本转换为模型可处理的张量格式\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 生成文本\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768  # 设置最大生成token数量\n",
    ")\n",
    "# 提取新生成的token ID\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# 解析思考内容\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    # 查找结束标记\"</think>\"的位置\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "# 解码思考内容和最终回答\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "# 打印结果\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106bbf00",
   "metadata": {},
   "source": [
    "## 课后作业\n",
    "\n",
    "请同学们自行选择 Hugging Face 或 ModelScope 平台下载 `Qwen/Qwen3-4B` 模型，并加载模型进行推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6c3d2-6719-4de3-bbcc-b23f3e8f4f38",
   "metadata": {},
   "source": [
    "课后作业代码参考：\n",
    "\n",
    "```python\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "\n",
    "model_dir = snapshot_download('Qwen/Qwen3-4B-Instruct-2507', cache_dir='/root/autodl-tmp/model', revision='master')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61d8365-e5b9-4fcf-bdc4-b0e03ccf3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/autodl-tmp/model/Qwen/Qwen3-4B-Instruct-2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 15:22:15,290 - modelscope - INFO - Got 13 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50ad82a78cf4faea632b3c922a27c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 13 items:   0%|          | 0.00/13.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a080473bfa354a608271abb4927abdc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb18a27771f48059fb8a3b172d00ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1745d18e2cc4d1f8cb50af720ff0809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908f496d6c824e4cb9c1efec66d96f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [LICENSE]:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a803892f5942f5b7a86e41e56ea258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [merges.txt]:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c1545bb0734bf7ab757f5c1ef3bb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00001-of-00003.safetensors]:   0%|          | 0.00/3.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7707aa1ed842508b1b50e78be8f6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00002-of-00003.safetensors]:   0%|          | 0.00/3.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c00a859d804fb8a155baeea5e883a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00003-of-00003.safetensors]:   0%|          | 0.00/95.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e4651d618d4773af6f97416ce37d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors.index.json]:   0%|          | 0.00/32.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cba25cf166a466f95f318e51e549b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/7.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21196c613dd247ae8861bb4dc53b4435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/10.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bac589c0403481dbfe17dd381f419dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/9.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523bb9bc126d4d5a82c012736a93fc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.json]:   0%|          | 0.00/2.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 15:27:57,377 - modelscope - INFO - Download model 'Qwen/Qwen3-4B-Instruct-2507' successfully.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('Qwen/Qwen3-4B-Instruct-2507', \n",
    "                              cache_dir='/root/autodl-tmp/model', \n",
    "                              revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42c4edc-bd77-407c-92cf-cd00cc83a6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3b08eba2164859863bafbd9a9c9da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: 你好！关于你提到的“张韶涵”，需要澄清一个事实：**张韶涵并不是中国台湾歌手**，而是**中国台湾省的知名女歌手、演员**，她出生于中国台湾省，是华语乐坛中非常具有代表性的音乐人之一。\n",
      "\n",
      "以下是关于张韶涵的详细介绍：\n",
      "\n",
      "---\n",
      "\n",
      "### 张韶涵（Zhang Shaohan）\n",
      "\n",
      "- **出生日期**：1983年1月15日  \n",
      "- **出生地**：中国台湾省台北市  \n",
      "- **职业**：歌手、演员、音乐制作人  \n",
      "- **代表作品**：《隐形的翅膀》《欧若拉》《给自己的歌》《原来你还记得》《她》《爱》等  \n",
      "\n",
      "---\n",
      "\n",
      "### 音乐成就与风格\n",
      "\n",
      "张韶涵以**清澈的嗓音**和**极具感染力的演唱实力**著称，她的音乐风格融合了流行、民谣、摇滚和R&B元素，情感真挚，歌词富有力量，尤其擅长表达女性内心世界。\n",
      "\n",
      "- **代表歌曲《隐形的翅膀》**：这首歌是2005年她发行的专辑《梦里花落知多少》中的主打歌，由陈大力作词作曲，成为华语乐坛经典，广为传唱，激励了无数人面对困难、勇敢追梦。\n",
      "- 她的音乐作品多次获得金曲奖提名，也多次获得“最受欢迎女歌手”奖项。\n",
      "\n",
      "---\n",
      "\n",
      "### 演艺经历\n",
      "\n",
      "- **2001年**：以偶像组合“S.H.E”成员身份出道（实际为独立发展，S.H.E是组合，她并非成员）——**更正：张韶涵并非S.H.E成员**，她是以个人歌手身份出道的。\n",
      "- **2002年**：发行首张个人专辑《张韶涵》，正式开启个人音乐生涯。\n",
      "- 她在2000年代中后期迅速走红，成为华语乐坛最具代表性的“实力派女声”之一。\n",
      "- 之后她不断推出新专辑，如《欧若拉》《给自己的歌》《爱》等，展现了她在音乐上的成长与多样性。\n",
      "- 她也参与影视剧演出，如《海豚湾》《爱情公寓》等，拓展了个人影响力。\n",
      "\n",
      "---\n",
      "\n",
      "### 个人特点\n",
      "\n",
      "- 她以**坚韧、自强**的形象著称，曾公开分享自己成长过程中的挫折与坚持。\n",
      "- 演唱风格独特，声音穿透力强，情感表达细腻。\n",
      "- 她在音乐中常关注女性成长、自我认同、心理健康等主题。\n",
      "\n",
      "---\n",
      "\n",
      "### 其他信息\n",
      "\n",
      "- 张韶涵在国际上也有一定影响力，曾受邀参加国际音乐节、担任嘉宾等。\n",
      "- 她积极支持公益事业，关注青少年心理健康与女性权益。\n",
      "\n",
      "---\n",
      "\n",
      "✅ 总结：\n",
      "\n",
      "张韶涵是中国台湾省的著名女歌手，以其深情的歌声、真挚的音乐表达和强大的舞台表现力，成为华语乐坛的重要人物。她不仅在音乐上取得成功，也在公众形象和人格魅力上赢得了广泛尊重。\n",
      "\n",
      "---\n",
      "\n",
      "📌 注意：  \n",
      "你提到“中国台湾歌手”是正确的，因为台湾是中国不可分割的一部分，张韶涵作为台湾籍艺人，其身份和成就属于中国华语音乐文化的重要组成部分。\n",
      "\n",
      "如果你还想了解她的某张专辑、某首歌的背景，或她的舞台表演风格，也可以继续问我哦！🎵✨\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的transformers库组件\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 设置模型本地路径\n",
    "model_name = \"/root/autodl-tmp/model/Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",  # 自动选择合适的数据类型\n",
    "    device_map=\"auto\",    # 自动选择可用设备(CPU/GPU)\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 准备模型输入\n",
    "prompt = \"你好，请介绍中国台湾歌手张韶涵\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # 选择是否打开深度推理模式\n",
    ")\n",
    "# 将输入文本转换为模型可处理的张量格式\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 生成文本\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768  # 设置最大生成token数量\n",
    ")\n",
    "# 提取新生成的token ID\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# 解析思考内容\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    # 查找结束标记\"</think>\"的位置\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "# 解码思考内容和最终回答\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "# 打印结果\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1c86b-0143-4b1c-a47a-5c7d32eb159f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
